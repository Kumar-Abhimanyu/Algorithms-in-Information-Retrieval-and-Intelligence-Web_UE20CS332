{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0TqQ8b2UlNM",
        "outputId": "dc789a3c-f28e-4717-a0f5-ac9040b21df9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"The cat sat on the mat, grooming its fur. It had just finished eating a mouse and was now relaxing in the warm sunlight that was streaming through the window. The cat's tail twitched every so often as it surveyed its kingdom, the room it lived in. Suddenly, a noise from outside caught its attention and it leapt up, ready to defend its territory.\"\n",
        "tokens = word_tokenize(sentence)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EzjRrenWVjQF",
        "outputId": "a68cbebc-1a2e-4eb0-c309-e42f9fee3f0b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'cat', 'sat', 'on', 'the', 'mat', ',', 'grooming', 'its', 'fur', '.', 'It', 'had', 'just', 'finished', 'eating', 'a', 'mouse', 'and', 'was', 'now', 'relaxing', 'in', 'the', 'warm', 'sunlight', 'that', 'was', 'streaming', 'through', 'the', 'window', '.', 'The', 'cat', \"'s\", 'tail', 'twitched', 'every', 'so', 'often', 'as', 'it', 'surveyed', 'its', 'kingdom', ',', 'the', 'room', 'it', 'lived', 'in', '.', 'Suddenly', ',', 'a', 'noise', 'from', 'outside', 'caught', 'its', 'attention', 'and', 'it', 'leapt', 'up', ',', 'ready', 'to', 'defend', 'its', 'territory', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " nltk.download('wordnet')\n",
        " nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fq66MzQabpB6",
        "outputId": "a4dcd8f6-3362-4b8b-e63e-19212a60422f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Lemmatization\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "print(lemmatized_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4zGcLOG5WeVo",
        "outputId": "d6d9ed92-7ff4-46e4-8faf-417b8f852744"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'cat', 'sat', 'on', 'the', 'mat', ',', 'grooming', 'it', 'fur', '.', 'It', 'had', 'just', 'finished', 'eating', 'a', 'mouse', 'and', 'wa', 'now', 'relaxing', 'in', 'the', 'warm', 'sunlight', 'that', 'wa', 'streaming', 'through', 'the', 'window', '.', 'The', 'cat', \"'s\", 'tail', 'twitched', 'every', 'so', 'often', 'a', 'it', 'surveyed', 'it', 'kingdom', ',', 'the', 'room', 'it', 'lived', 'in', '.', 'Suddenly', ',', 'a', 'noise', 'from', 'outside', 'caught', 'it', 'attention', 'and', 'it', 'leapt', 'up', ',', 'ready', 'to', 'defend', 'it', 'territory', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Stemming using Porter Stemmer\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
        "print(stemmed_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UiePsPpxWqne",
        "outputId": "3718a91f-018b-4b4a-8190-a0ffeb230db0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the', 'cat', 'sat', 'on', 'the', 'mat', ',', 'groom', 'it', 'fur', '.', 'it', 'had', 'just', 'finish', 'eat', 'a', 'mous', 'and', 'wa', 'now', 'relax', 'in', 'the', 'warm', 'sunlight', 'that', 'wa', 'stream', 'through', 'the', 'window', '.', 'the', 'cat', \"'s\", 'tail', 'twitch', 'everi', 'so', 'often', 'as', 'it', 'survey', 'it', 'kingdom', ',', 'the', 'room', 'it', 'live', 'in', '.', 'suddenli', ',', 'a', 'nois', 'from', 'outsid', 'caught', 'it', 'attent', 'and', 'it', 'leapt', 'up', ',', 'readi', 'to', 'defend', 'it', 'territori', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Stemming using Snowball Stemmer\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "\n",
        "stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
        "print(stemmed_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lje67kWKcBi1",
        "outputId": "a0288759-77f1-43b3-fb48-f31057166e2c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the', 'cat', 'sat', 'on', 'the', 'mat', ',', 'groom', 'it', 'fur', '.', 'it', 'had', 'just', 'finish', 'eat', 'a', 'mous', 'and', 'was', 'now', 'relax', 'in', 'the', 'warm', 'sunlight', 'that', 'was', 'stream', 'through', 'the', 'window', '.', 'the', 'cat', \"'s\", 'tail', 'twitch', 'everi', 'so', 'often', 'as', 'it', 'survey', 'it', 'kingdom', ',', 'the', 'room', 'it', 'live', 'in', '.', 'sudden', ',', 'a', 'nois', 'from', 'outsid', 'caught', 'it', 'attent', 'and', 'it', 'leapt', 'up', ',', 'readi', 'to', 'defend', 'it', 'territori', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Posting list\n",
        "from collections import defaultdict\n",
        "\n",
        "# Initialize the posting list\n",
        "posting_list = defaultdict(list)\n",
        "\n",
        "# Add some documents to the posting list\n",
        "doc1 = \"The quick brown fox jumps over the lazy dog\"\n",
        "doc2 = \"The lazy dog is lying in the sun\"\n",
        "doc3 = \"The quick brown fox is smart\"\n",
        "\n",
        "# Tokenize the documents into a list of words\n",
        "doc1_tokens = doc1.split()\n",
        "doc2_tokens = doc2.split()\n",
        "doc3_tokens = doc3.split()\n",
        "\n",
        "# Iterate through each document and add the tokens to the posting list\n",
        "for i, doc in enumerate([doc1_tokens, doc2_tokens, doc3_tokens]):\n",
        "    for j, token in enumerate(doc):\n",
        "        posting_list[token].append((i, j))\n",
        "\n",
        "# Print the posting list\n",
        "print(posting_list)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3h0gAMC_caDV",
        "outputId": "9186735b-dc28-4445-bd06-bc24df5eebed"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "defaultdict(<class 'list'>, {'The': [(0, 0), (1, 0), (2, 0)], 'quick': [(0, 1), (2, 1)], 'brown': [(0, 2), (2, 2)], 'fox': [(0, 3), (2, 3)], 'jumps': [(0, 4)], 'over': [(0, 5)], 'the': [(0, 6), (1, 6)], 'lazy': [(0, 7), (1, 1)], 'dog': [(0, 8), (1, 2)], 'is': [(1, 3), (2, 4)], 'lying': [(1, 4)], 'in': [(1, 5)], 'sun': [(1, 7)], 'smart': [(2, 5)]})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4MP-uPwfd20g"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}